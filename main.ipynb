{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Modelling Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 03:17:28.624148: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 03:17:29.073925: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2022-09-23 03:17:29.604531: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 03:17:29.604575: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2022-09-23 03:17:29.604580: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----SUCCESS: LOADED DATASET----\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.data_load import load_test, load_train\n",
    "from utils.solution import create_solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 03:22:39.450638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 03:22:39.510724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 03:22:39.510923: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "# gpu test\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dogs 501\n",
      "Processed 100/500\n",
      "Processed 200/500\n",
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "cows 500\n",
      "Processed 100/500\n",
      "Processed 200/500\n",
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "spiders 500\n",
      "Processed 100/500\n",
      "Processed 200/500\n",
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "fishes 500\n",
      "Processed 100/500\n",
      "Processed 200/500\n",
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "butterfiles 500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 100/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 200/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 300/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 400/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n",
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/500\n",
      "horses 500\n",
      "Processed 100/500\n",
      "Processed 200/500\n",
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "chickens 500\n",
      "Processed 100/500\n",
      "Processed 200/500\n",
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "elephants 500\n",
      "Processed 100/500\n",
      "Processed 200/500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "libpng warning: iCCP: known incorrect sRGB profile\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "lions 500\n",
      "Processed 100/500\n",
      "Processed 200/500\n",
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "cats 501\n",
      "Processed 100/500\n",
      "Processed 200/500\n",
      "Processed 300/500\n",
      "Processed 400/500\n",
      "Processed 500/500\n",
      "----SUCCESS: LOADED DATASET----\n"
     ]
    }
   ],
   "source": [
    "dm = load_train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. Default FIT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_cnn():\n",
    "    model = models.Sequential()\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', activation= 'relu', input_shape=(32,32,3)))\n",
    "    model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu'))\n",
    "    model.add(layers.AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "    model.add(layers.AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
    "    model.add(layers.Flatten())\n",
    "    model.add(layers.Dense(10, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_cnn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate= 0.001)\n",
    "model.compile(optimizer=opt, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "history = model.fit(x = dm.X_train, y = dm.y_train, validation_data = (dm.X_valid, dm.y_valid), epochs = 15, batch_size = 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_solution(model, name=\"default-trained\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. Assignment Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.models import DefaultModel\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model1(DefaultModel):\n",
    "    def __init__(self,\n",
    "                 name='network1',\n",
    "                 width=32, height=32, depth=3,\n",
    "                 num_blocks=3,\n",
    "                 feature_maps=32,\n",
    "                 num_classes=10, \n",
    "                 drop_rate=0.2,\n",
    "                 batch_norm = True,\n",
    "                 is_augmentation = False,\n",
    "                 activation_func='relu',\n",
    "                 use_skip = True,\n",
    "                 optimizer='adam',\n",
    "                 batch_size= 32,\n",
    "                 num_epochs= 20,\n",
    "                 learning_rate=0.001,\n",
    "                 verbose= True, \n",
    "                 es_patience = 3):\n",
    "        super(Model1, self).__init__(name, width, height, depth, num_blocks, feature_maps, num_classes, drop_rate, batch_norm, is_augmentation, \n",
    "                                        activation_func, optimizer, batch_size, num_epochs, learning_rate, verbose)\n",
    "        self.use_skip = use_skip\n",
    "        self.es_patience = es_patience\n",
    "\n",
    "    def build_cnn(self):\n",
    "        \"\"\"Builds a dynamic CNN based on attributes passed into the class\"\"\"\n",
    "        self.model = models.Model()\n",
    "        inputs = layers.Input(shape=(self.width, self.height, self.depth))\n",
    "        h = inputs\n",
    "\n",
    "        for i in range(self.num_blocks):\n",
    "            h = self.ResidBlock(h=h, num_channels=self.feature_maps[i])\n",
    "\n",
    "        h = layers.Flatten()(h)\n",
    "        h = layers.Dense(units=self.num_classes, activation=\"softmax\")(h)\n",
    "\n",
    "        self.model = models.Model(inputs=inputs, outputs=h)\n",
    "        self.model.compile(optimizer=self.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    def ResidBlock(self, h, num_channels) -> list:\n",
    "        \"\"\"Passes input h through a residual block.\n",
    "        Args:\n",
    "            h (Matrix): The output of the previous layer\n",
    "            num_channes (int): The number of channels for our Conv3D layers\n",
    "        Returns:\n",
    "            h (Matrix): Output of final.\n",
    "        \"\"\"\n",
    "        h = layers.Conv2D(num_channels, (3,3), padding='same')(h)\n",
    "        skip_signal = h\n",
    "        \n",
    "        if self.batch_norm:\n",
    "            h = layers.BatchNormalization()(h)   \n",
    "\n",
    "        h = layers.Activation(self.activation_func)(h)\n",
    "        h = layers.Conv2D(num_channels, (3,3), padding='same')(h)\n",
    "\n",
    "        if self.batch_norm:\n",
    "            h = layers.BatchNormalization()(h)\n",
    "\n",
    "        if self.use_skip:\n",
    "            h = skip_signal + h\n",
    "\n",
    "        h = layers.Activation(self.activation_func)(h)\n",
    "        h = layers.AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='same')(h)\n",
    "        h = layers.Dropout(rate=self.drop_rate)(h)\n",
    "\n",
    "        return h\n",
    "    \n",
    "    def fit(self, data_manager, batch_size=None, num_epochs=None, es_patience=None):\n",
    "        batch_size = self.batch_size if batch_size is None else batch_size\n",
    "        num_epochs = self.num_epochs if num_epochs is None else num_epochs\n",
    "        es_patience = self.es_patience if es_patience is None else es_patience\n",
    "        self.model.compile(optimizer=self.optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        \n",
    "        early_checkpoint = EarlyStopping(patience=es_patience, monitor='val_accuracy', mode='max')\n",
    "        callbacks = [early_checkpoint]\n",
    "\n",
    "        if self.is_augmentation:\n",
    "            datagen = ImageDataGenerator(horizontal_flip=True, width_shift_range=0.1, height_shift_range=0.1)\n",
    "            datagen.fit(data_manager.X_train)\n",
    "            X_train_aug = datagen.flow(data_manager.X_train).x\n",
    "            \n",
    "            self.history = self.model.fit(x = X_train_aug, y = data_manager.y_train, validation_data = (data_manager.X_valid, data_manager.y_valid), \n",
    "                epochs = num_epochs, batch_size = batch_size, verbose= self.verbose, callbacks=callbacks)\n",
    "\n",
    "        else:\n",
    "            self.history = self.model.fit(x = data_manager.X_train, y = data_manager.y_train, validation_data = (data_manager.X_valid, data_manager.y_valid), \n",
    "                epochs = num_epochs, batch_size = batch_size, verbose= self.verbose, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 03:22:54.648123: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-09-23 03:22:54.648928: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 03:22:54.649283: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 03:22:54.649604: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 03:22:59.440608: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 03:22:59.440787: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 03:22:59.440927: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:980] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-09-23 03:22:59.441042: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1616] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5002 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3070 Ti, pci bus id: 0000:08:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 32, 32, 32)   896         ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization (BatchNorm  (None, 32, 32, 32)  128         ['conv2d[0][0]']                 \n",
      " alization)                                                                                       \n",
      "                                                                                                  \n",
      " activation (Activation)        (None, 32, 32, 32)   0           ['batch_normalization[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 32, 32, 32)   9248        ['activation[0][0]']             \n",
      "                                                                                                  \n",
      " batch_normalization_1 (BatchNo  (None, 32, 32, 32)  128         ['conv2d_1[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add (TFOpLamb  (None, 32, 32, 32)  0           ['conv2d[0][0]',                 \n",
      " da)                                                              'batch_normalization_1[0][0]']  \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 32, 32, 32)   0           ['tf.__operators__.add[0][0]']   \n",
      "                                                                                                  \n",
      " average_pooling2d (AveragePool  (None, 16, 16, 32)  0           ['activation_1[0][0]']           \n",
      " ing2D)                                                                                           \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (None, 16, 16, 32)   0           ['average_pooling2d[0][0]']      \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 16, 16, 64)   18496       ['dropout[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_2 (BatchNo  (None, 16, 16, 64)  256         ['conv2d_2[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 16, 16, 64)   0           ['batch_normalization_2[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_3 (Conv2D)              (None, 16, 16, 64)   36928       ['activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_3 (BatchNo  (None, 16, 16, 64)  256         ['conv2d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_1 (TFOpLa  (None, 16, 16, 64)  0           ['conv2d_2[0][0]',               \n",
      " mbda)                                                            'batch_normalization_3[0][0]']  \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 16, 16, 64)   0           ['tf.__operators__.add_1[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_1 (AveragePo  (None, 8, 8, 64)    0           ['activation_3[0][0]']           \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (None, 8, 8, 64)     0           ['average_pooling2d_1[0][0]']    \n",
      "                                                                                                  \n",
      " conv2d_4 (Conv2D)              (None, 8, 8, 128)    73856       ['dropout_1[0][0]']              \n",
      "                                                                                                  \n",
      " batch_normalization_4 (BatchNo  (None, 8, 8, 128)   512         ['conv2d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " activation_4 (Activation)      (None, 8, 8, 128)    0           ['batch_normalization_4[0][0]']  \n",
      "                                                                                                  \n",
      " conv2d_5 (Conv2D)              (None, 8, 8, 128)    147584      ['activation_4[0][0]']           \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 8, 8, 128)   512         ['conv2d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " tf.__operators__.add_2 (TFOpLa  (None, 8, 8, 128)   0           ['conv2d_4[0][0]',               \n",
      " mbda)                                                            'batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " activation_5 (Activation)      (None, 8, 8, 128)    0           ['tf.__operators__.add_2[0][0]'] \n",
      "                                                                                                  \n",
      " average_pooling2d_2 (AveragePo  (None, 4, 4, 128)   0           ['activation_5[0][0]']           \n",
      " oling2D)                                                                                         \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 4, 4, 128)    0           ['average_pooling2d_2[0][0]']    \n",
      "                                                                                                  \n",
      " flatten (Flatten)              (None, 2048)         0           ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 10)           20490       ['flatten[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 309,290\n",
      "Trainable params: 308,394\n",
      "Non-trainable params: 896\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "bm1 = Model1()\n",
    "bm1.build_cnn()\n",
    "bm1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-09-23 03:23:12.311953: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8500\n",
      "2022-09-23 03:23:23.240206: I tensorflow/stream_executor/cuda/cuda_blas.cc:1614] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 22s 8ms/step - loss: 1.9116 - accuracy: 0.3385 - val_loss: 2.2883 - val_accuracy: 0.1660\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 1.5048 - accuracy: 0.4845 - val_loss: 2.6891 - val_accuracy: 0.2180\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 1.3043 - accuracy: 0.5543 - val_loss: 1.7104 - val_accuracy: 0.4020\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 1.1161 - accuracy: 0.6105 - val_loss: 1.8213 - val_accuracy: 0.4280\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 1.0095 - accuracy: 0.6492 - val_loss: 1.8143 - val_accuracy: 0.4860\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.8871 - accuracy: 0.6995 - val_loss: 1.3258 - val_accuracy: 0.5540\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.7986 - accuracy: 0.7283 - val_loss: 1.6925 - val_accuracy: 0.5060\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.6999 - accuracy: 0.7590 - val_loss: 1.1948 - val_accuracy: 0.6260\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.6174 - accuracy: 0.7872 - val_loss: 1.2645 - val_accuracy: 0.6060\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.5482 - accuracy: 0.8090 - val_loss: 1.1659 - val_accuracy: 0.6300\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.4938 - accuracy: 0.8328 - val_loss: 1.2973 - val_accuracy: 0.6060\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.4338 - accuracy: 0.8525 - val_loss: 1.2732 - val_accuracy: 0.6420\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.3822 - accuracy: 0.8717 - val_loss: 1.1785 - val_accuracy: 0.6620\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 1s 6ms/step - loss: 0.3241 - accuracy: 0.8852 - val_loss: 1.2301 - val_accuracy: 0.6520\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.2864 - accuracy: 0.9060 - val_loss: 1.3714 - val_accuracy: 0.6240\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 1s 5ms/step - loss: 0.2765 - accuracy: 0.9065 - val_loss: 1.5018 - val_accuracy: 0.5820\n"
     ]
    }
   ],
   "source": [
    "bm1.fit(dm, batch_size=32, num_epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/16 [==============================] - 0s 2ms/step - loss: 1.4110 - accuracy: 0.6000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6000000238418579"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bm1.compute_accuracy(dm.X_test, dm.y_test, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "722/722 [==============================] - 9s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "create_solution(bm1.model, name=\"bm1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Grid Search Model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO | Fitting Model w params: {'num_blocks': 3, 'is_augmentation': True, 'verbose': False}\n",
      "SUCCESS | Model Fitted\n",
      "8/8 [==============================] - 0s 6ms/step - loss: 1.2878 - accuracy: 0.5980\n",
      "INFO | Fitting Model w params: {'num_blocks': 3, 'is_augmentation': False, 'verbose': False}\n",
      "SUCCESS | Model Fitted\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.2365 - accuracy: 0.6400\n",
      "INFO | Fitting Model w params: {'num_blocks': 4, 'is_augmentation': True, 'verbose': False}\n",
      "SUCCESS | Model Fitted\n",
      "8/8 [==============================] - 0s 3ms/step - loss: 1.3783 - accuracy: 0.6060\n",
      "INFO | Fitting Model w params: {'num_blocks': 4, 'is_augmentation': False, 'verbose': False}\n",
      "SUCCESS | Model Fitted\n",
      "8/8 [==============================] - 0s 2ms/step - loss: 1.3579 - accuracy: 0.5960\n",
      "Best Params:\n",
      "index                    1\n",
      "num_blocks               3\n",
      "is_augmentation      False\n",
      "verbose              False\n",
      "val_accuracy         0.674\n",
      "train_accuracy     0.89075\n",
      "test_accuracy         0.64\n",
      "Name: 0, dtype: object\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>num_blocks</th>\n",
       "      <th>is_augmentation</th>\n",
       "      <th>verbose</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>test_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.674</td>\n",
       "      <td>0.89075</td>\n",
       "      <td>0.640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.616</td>\n",
       "      <td>0.90550</td>\n",
       "      <td>0.606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>0.600</td>\n",
       "      <td>0.83150</td>\n",
       "      <td>0.598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.87750</td>\n",
       "      <td>0.596</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  num_blocks  is_augmentation  verbose  val_accuracy  train_accuracy  \\\n",
       "0      1           3            False    False         0.674         0.89075   \n",
       "1      2           4             True    False         0.616         0.90550   \n",
       "2      0           3             True    False         0.600         0.83150   \n",
       "3      3           4            False    False         0.634         0.87750   \n",
       "\n",
       "   test_accuracy  \n",
       "0          0.640  \n",
       "1          0.606  \n",
       "2          0.598  \n",
       "3          0.596  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from itertools import product\n",
    "\n",
    "\n",
    "def grid_search_kwargs(grid, obj_class, verbose=True):\n",
    "    \"\"\"Self Implemented Grid Search\"\"\"\n",
    "    all_results = []\n",
    "    for pv in list(product(*grid.values())):\n",
    "        results = {}\n",
    "        params = {k:p for p, k in zip(pv, list(grid.keys()))}\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"INFO | Fitting Model w params: {params}\")\n",
    "\n",
    "        mod = obj_class(**params)\n",
    "        mod.build_cnn()\n",
    "        mod.fit(dm)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"SUCCESS | Model Fitted\")\n",
    "\n",
    "        results.update(params)\n",
    "\n",
    "        results['val_accuracy'] = mod.history.history['val_accuracy'][-1]\n",
    "        results['train_accuracy'] = mod.history.history['accuracy'][-1]\n",
    "        results['test_accuracy'] = mod.compute_accuracy(dm.X_test, dm.y_test)\n",
    "\n",
    "        all_results.append(results)\n",
    "\n",
    "\n",
    "    df = pd.DataFrame(all_results).sort_values('test_accuracy', ascending=False).reset_index()\n",
    "    print(\"Best Params:\")\n",
    "    print(df.loc[0])\n",
    "\n",
    "    return df\n",
    "\n",
    "grid = {\n",
    "    \"num_blocks\": [3,4,5],\n",
    "    \"is_augmentation\": [True, False],\n",
    "    \"batch_size\": [16, 32],\n",
    "    \"es_patience\": [3,6]\n",
    "}\n",
    "\n",
    "test_grid = {\n",
    "    \"num_blocks\": [3,4],\n",
    "    \"is_augmentation\": [True, False],\n",
    "    \"verbose\": [False]\n",
    "}\n",
    "\n",
    "res = grid_search_kwargs(test_grid, Model1)\n",
    "res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('deep_learning')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "744d38de6d85aac356f38ea46202bc135fd7f6bffbfa19cc48ec10c4285a4206"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
